{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from code.mnist_NA import *\n",
    "from code.helper_FMP import *\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from tqdm import tnrange,tqdm_notebook\n",
    "import os \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 16\n",
    "\n",
    "logs_path = \"tensorflow_logs/mnist_logs\"\n",
    "model_path  = \"/models/mnist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/prakash/benchmark'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filelist_train = glob.glob(os.getcwd()+\"/train/*.png\")\n",
    "filelist_test = glob.glob(os.getcwd()+\"/test/*.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/prakash/benchmark/train/11934.png'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filelist_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train= np.array([image_read(filelist_train[i]) for i in range(len(filelist_train))])\n",
    "test = np.array([image_read(filelist_test[i]) for i in range(len(filelist_test))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49000, 36, 36, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWUAAAFgCAYAAABjSGgIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhJAcQVEmhJAc8f8BfMbEnHr9saoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7523fe96a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train[100,:,:,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  1.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  1.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "labels = pd.read_table(os.getcwd()+\"/train.csv\",sep=\",\",index_col = [\"filename\"])\n",
    "labels = labels.ix[list(map(int, ([x.rsplit('/',1)[-1].rsplit('.',1)[0] for x in filelist_train])))]\n",
    "labels = pd.get_dummies(labels['label'])\n",
    "labels = labels.as_matrix()\n",
    "print (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trX,valX,trY, valY = train_test_split(train,labels,\n",
    "\ttest_size = 0.20, random_state = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model building \n",
    "X = tf.placeholder(tf.float32,[None,36,36,3],name = \"Input_data\")\n",
    "Y = tf.placeholder(tf.float32,[None,10], name = \"InputLabels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = {\n",
    "\"w1\":init_weights([2,2,3,32]),\n",
    "\"w2\":init_weights([2,2,32,64]),\n",
    "\"w3\":init_weights([2,2,64,96]),\n",
    "\"w4\":init_weights([2,2,96,128]),\n",
    "\"w5\":init_weights([2,2,128,160]),\n",
    "\"w6\":init_weights([2,2,160,192]),\n",
    "\"w7\":init_weights([1,1,192,192]),\n",
    "\"w8\":init_weights([1,1,192,192]),\n",
    "\"w9\":init_weights([1728,10])\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "bias = {\n",
    "\"b1\": init_bias([32]),\n",
    "\"b2\": init_bias([64]),\n",
    "\"b3\": init_bias([96]),\n",
    "\"b4\": init_bias([128]),\n",
    "\"b5\": init_bias([160]),\n",
    "\"b6\": init_bias([192]),\n",
    "\"b7\": init_bias([192]),\n",
    "\"b8\": init_bias([192]),\n",
    "\"b9\": init_bias([10])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = model(X,weights,bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Add:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred,Y))\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=0.00001).minimize(cost)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluate the model\n",
    "correct_pred= tf.equal(tf.argmax(pred,1),tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n",
    "\n",
    "#Initialize all the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create summary to monitor cost tensor , accuracy tensor\n",
    "tf.scalar_summary(\"loss\",cost)\n",
    "tf.scalar_summary(\"accuracy\",accuracy)\n",
    "\n",
    "merged_summary_op = tf.merge_all_summaries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 Train loss =2.302590 Train acc =0.098000 valid loss =2.302548 valid acc =0.105000\n",
      "0 1000 Train loss =2.301074 Train acc =0.115000 valid loss =2.300365 valid acc =0.104000\n",
      "0 2000 Train loss =2.299824 Train acc =0.115000 valid loss =2.299810 valid acc =0.104000\n",
      "1 0 Train loss =1.841220 Train acc =0.263000 valid loss =1.641279 valid acc =0.381000\n",
      "1 1000 Train loss =0.796329 Train acc =0.755000 valid loss =1.007066 valid acc =0.672000\n",
      "1 2000 Train loss =0.366906 Train acc =0.876000 valid loss =0.358619 valid acc =0.887000\n",
      "2 0 Train loss =0.331030 Train acc =0.900000 valid loss =0.424946 valid acc =0.877000\n",
      "2 1000 Train loss =0.413063 Train acc =0.874000 valid loss =0.341812 valid acc =0.884000\n",
      "2 2000 Train loss =0.207069 Train acc =0.933000 valid loss =0.171587 valid acc =0.951000\n",
      "3 0 Train loss =0.206459 Train acc =0.940000 valid loss =0.305751 valid acc =0.908000\n",
      "3 1000 Train loss =0.360316 Train acc =0.878000 valid loss =0.280711 valid acc =0.921000\n",
      "3 2000 Train loss =0.145471 Train acc =0.956000 valid loss =0.181585 valid acc =0.942000\n",
      "4 0 Train loss =0.138626 Train acc =0.958000 valid loss =0.167789 valid acc =0.946000\n",
      "4 1000 Train loss =0.139366 Train acc =0.948000 valid loss =0.172249 valid acc =0.954000\n",
      "4 2000 Train loss =0.145994 Train acc =0.956000 valid loss =0.199444 valid acc =0.939000\n",
      "5 0 Train loss =0.145176 Train acc =0.953000 valid loss =0.079596 valid acc =0.974000\n",
      "5 1000 Train loss =0.125590 Train acc =0.961000 valid loss =0.111152 valid acc =0.968000\n",
      "5 2000 Train loss =0.082376 Train acc =0.973000 valid loss =0.147328 valid acc =0.957000\n",
      "6 0 Train loss =0.136469 Train acc =0.957000 valid loss =0.122231 valid acc =0.962000\n",
      "6 1000 Train loss =0.120012 Train acc =0.963000 valid loss =0.084920 valid acc =0.974000\n",
      "6 2000 Train loss =0.184922 Train acc =0.946000 valid loss =0.187965 valid acc =0.948000\n",
      "7 0 Train loss =0.220569 Train acc =0.944000 valid loss =0.286434 valid acc =0.938000\n",
      "7 1000 Train loss =0.208586 Train acc =0.955000 valid loss =0.125885 valid acc =0.964000\n",
      "7 2000 Train loss =0.147682 Train acc =0.951000 valid loss =0.224890 valid acc =0.939000\n",
      "8 0 Train loss =0.112716 Train acc =0.967000 valid loss =0.088840 valid acc =0.974000\n",
      "8 1000 Train loss =0.377794 Train acc =0.916000 valid loss =0.117538 valid acc =0.967000\n",
      "8 2000 Train loss =0.114755 Train acc =0.967000 valid loss =0.170072 valid acc =0.954000\n",
      "9 0 Train loss =0.218554 Train acc =0.942000 valid loss =0.132698 valid acc =0.962000\n",
      "9 1000 Train loss =0.126135 Train acc =0.965000 valid loss =0.102990 valid acc =0.976000\n",
      "9 2000 Train loss =0.148790 Train acc =0.955000 valid loss =0.256409 valid acc =0.940000\n",
      "10 0 Train loss =0.138364 Train acc =0.955000 valid loss =0.141246 valid acc =0.960000\n",
      "10 1000 Train loss =0.089571 Train acc =0.982000 valid loss =0.153097 valid acc =0.961000\n",
      "10 2000 Train loss =0.165956 Train acc =0.949000 valid loss =0.138130 valid acc =0.957000\n",
      "11 0 Train loss =0.074966 Train acc =0.977000 valid loss =0.084340 valid acc =0.979000\n",
      "11 1000 Train loss =0.425991 Train acc =0.905000 valid loss =0.321534 valid acc =0.929000\n",
      "11 2000 Train loss =0.255158 Train acc =0.933000 valid loss =0.146499 valid acc =0.967000\n",
      "12 0 Train loss =0.170207 Train acc =0.948000 valid loss =0.115035 valid acc =0.971000\n",
      "12 1000 Train loss =0.143593 Train acc =0.960000 valid loss =0.124272 valid acc =0.966000\n",
      "12 2000 Train loss =0.127139 Train acc =0.969000 valid loss =0.157504 valid acc =0.966000\n",
      "13 0 Train loss =0.456398 Train acc =0.900000 valid loss =0.217414 valid acc =0.943000\n",
      "13 1000 Train loss =0.134507 Train acc =0.961000 valid loss =0.122006 valid acc =0.961000\n",
      "13 2000 Train loss =0.270920 Train acc =0.929000 valid loss =0.136505 valid acc =0.956000\n",
      "14 0 Train loss =0.125307 Train acc =0.959000 valid loss =0.117574 valid acc =0.968000\n",
      "14 1000 Train loss =0.149475 Train acc =0.952000 valid loss =0.110732 valid acc =0.966000\n",
      "14 2000 Train loss =0.156937 Train acc =0.947000 valid loss =0.094337 valid acc =0.968000\n",
      "15 0 Train loss =0.065098 Train acc =0.981000 valid loss =0.153347 valid acc =0.957000\n",
      "15 1000 Train loss =0.129281 Train acc =0.964000 valid loss =0.069163 valid acc =0.978000\n",
      "15 2000 Train loss =0.118524 Train acc =0.970000 valid loss =0.128770 valid acc =0.968000\n",
      "16 0 Train loss =0.474253 Train acc =0.884000 valid loss =0.148492 valid acc =0.960000\n",
      "16 1000 Train loss =0.068222 Train acc =0.983000 valid loss =0.119991 valid acc =0.967000\n",
      "16 2000 Train loss =0.121803 Train acc =0.968000 valid loss =0.118553 valid acc =0.968000\n",
      "17 0 Train loss =0.179951 Train acc =0.948000 valid loss =0.089208 valid acc =0.977000\n",
      "17 1000 Train loss =0.075063 Train acc =0.982000 valid loss =0.116045 valid acc =0.969000\n",
      "17 2000 Train loss =0.306355 Train acc =0.946000 valid loss =0.079744 valid acc =0.979000\n",
      "18 0 Train loss =0.112199 Train acc =0.965000 valid loss =0.122680 valid acc =0.963000\n",
      "18 1000 Train loss =2.987553 Train acc =0.350000 valid loss =2.594521 valid acc =0.412000\n",
      "18 2000 Train loss =1.443858 Train acc =0.561000 valid loss =1.390448 valid acc =0.584000\n",
      "19 0 Train loss =1.152384 Train acc =0.612000 valid loss =1.399642 valid acc =0.591000\n",
      "19 1000 Train loss =1.474628 Train acc =0.522000 valid loss =0.797622 valid acc =0.729000\n",
      "19 2000 Train loss =0.817149 Train acc =0.730000 valid loss =0.871801 valid acc =0.706000\n",
      "\n",
      "Optimization finished\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    summary_writer = tf.train.SummaryWriter(logs_path,graph=tf.get_default_graph())\n",
    "    val_acc = [0]\n",
    "    for ep in tqdm_notebook(range(epochs),desc=\"1st loop\"):\n",
    "        batches = int(len(trX)/batch_size)\n",
    "        x_train,y_train = shuffle(trX,trY,random_state=0)\n",
    "        for i in tqdm_notebook(range(batches),desc=\"2nd loop\", leave = False):\n",
    "            X_train, Y_train = x_train[i*batch_size:(i+1)*batch_size],y_train[i*batch_size:(i+1)*batch_size]\n",
    "            _,c,summary = sess.run([optimizer,cost,merged_summary_op],feed_dict={X : np.float32(X_train), Y : np.float32(Y_train)})\n",
    "            if i % 1000 == 0:\n",
    "                train_loss, train_acc = sess.run([cost,accuracy], feed_dict={X : np.float32(x_train[0:1000,:,:,:]), Y : np.float32(y_train[0:1000])})\n",
    "                valx,valy = shuffle(valX,valY, random_state=0)\n",
    "                valid_loss, valid_acc = sess.run([cost,accuracy], feed_dict={X : np.float32(valx[0:1000]), Y : np.float32(valy[0:1000])})\n",
    "                print (str(ep),str(i),\"Train loss =\"+\"{:.6f}\".format(train_loss),\\\n",
    "                       \"Train acc =\"+\"{:.6f}\".format(train_acc),\\\n",
    "                      \"valid loss =\"+\"{:.6f}\".format(valid_loss),\\\n",
    "                      \"valid acc =\"+\"{:.6f}\".format(valid_acc))\n",
    "                if max(val_acc) < valid_acc:\n",
    "                    val_acc.append(valid_acc)\n",
    "                    saver.save(sess,os.getcwd()+model_path+\"_\"+str(ep)+\"_\"+str(i))\n",
    "    print (\"Optimization finished\")\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0.105,\n",
       " 0.38100004,\n",
       " 0.67199999,\n",
       " 0.88700002,\n",
       " 0.95100003,\n",
       " 0.95400012,\n",
       " 0.97400004,\n",
       " 0.97600001,\n",
       " 0.97899997,\n",
       " 0.97900009]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reducing the learning rate and running more epochs \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 Train loss =0.098165 Train acc =0.967000 valid loss =0.151144 valid acc =0.959000\n",
      "0 1000 Train loss =0.139033 Train acc =0.970000 valid loss =0.231926 valid acc =0.951000\n",
      "0 2000 Train loss =0.078833 Train acc =0.973000 valid loss =0.163542 valid acc =0.953000\n",
      "1 0 Train loss =0.101336 Train acc =0.968000 valid loss =0.097532 valid acc =0.973000\n",
      "1 1000 Train loss =0.284841 Train acc =0.912000 valid loss =0.305751 valid acc =0.905000\n",
      "1 2000 Train loss =0.112236 Train acc =0.965000 valid loss =0.241569 valid acc =0.930000\n",
      "2 0 Train loss =0.147014 Train acc =0.953000 valid loss =0.161332 valid acc =0.959000\n",
      "2 1000 Train loss =0.474135 Train acc =0.860000 valid loss =0.502667 valid acc =0.859000\n",
      "2 2000 Train loss =0.242465 Train acc =0.927000 valid loss =0.248673 valid acc =0.939000\n",
      "3 0 Train loss =0.173770 Train acc =0.958000 valid loss =0.182960 valid acc =0.948000\n",
      "3 1000 Train loss =0.222229 Train acc =0.932000 valid loss =0.288344 valid acc =0.922000\n",
      "3 2000 Train loss =0.220576 Train acc =0.940000 valid loss =0.138238 valid acc =0.960000\n",
      "4 0 Train loss =0.149445 Train acc =0.953000 valid loss =0.124049 valid acc =0.954000\n",
      "4 1000 Train loss =0.181090 Train acc =0.950000 valid loss =0.222356 valid acc =0.940000\n",
      "4 2000 Train loss =0.078754 Train acc =0.980000 valid loss =0.083861 valid acc =0.969000\n",
      "5 0 Train loss =0.105111 Train acc =0.969000 valid loss =0.239197 valid acc =0.942000\n",
      "5 1000 Train loss =0.071360 Train acc =0.982000 valid loss =0.127181 valid acc =0.967000\n",
      "5 2000 Train loss =0.118832 Train acc =0.972000 valid loss =0.254780 valid acc =0.934000\n",
      "6 0 Train loss =0.155744 Train acc =0.955000 valid loss =0.131986 valid acc =0.961000\n",
      "6 1000 Train loss =0.073616 Train acc =0.975000 valid loss =0.120038 valid acc =0.971000\n",
      "6 2000 Train loss =0.133713 Train acc =0.963000 valid loss =0.190479 valid acc =0.965000\n",
      "7 0 Train loss =0.219823 Train acc =0.951000 valid loss =0.152246 valid acc =0.961000\n",
      "7 1000 Train loss =0.080018 Train acc =0.973000 valid loss =0.120829 valid acc =0.966000\n",
      "7 2000 Train loss =0.140416 Train acc =0.968000 valid loss =0.088550 valid acc =0.975000\n",
      "8 0 Train loss =0.135031 Train acc =0.954000 valid loss =0.111434 valid acc =0.965000\n",
      "8 1000 Train loss =0.086777 Train acc =0.975000 valid loss =0.091531 valid acc =0.974000\n",
      "8 2000 Train loss =0.136078 Train acc =0.959000 valid loss =0.110328 valid acc =0.967000\n",
      "9 0 Train loss =1.169949 Train acc =0.755000 valid loss =0.327543 valid acc =0.909000\n",
      "9 1000 Train loss =0.185299 Train acc =0.950000 valid loss =0.407814 valid acc =0.878000\n",
      "9 2000 Train loss =0.086188 Train acc =0.974000 valid loss =0.093844 valid acc =0.973000\n",
      "10 0 Train loss =0.103587 Train acc =0.968000 valid loss =0.159535 valid acc =0.957000\n",
      "10 1000 Train loss =0.179225 Train acc =0.944000 valid loss =0.121149 valid acc =0.962000\n",
      "10 2000 Train loss =0.075914 Train acc =0.978000 valid loss =0.073514 valid acc =0.983000\n",
      "11 0 Train loss =0.087411 Train acc =0.972000 valid loss =0.139642 valid acc =0.970000\n",
      "11 1000 Train loss =7.333161 Train acc =0.344000 valid loss =8.023623 valid acc =0.191000\n",
      "11 2000 Train loss =2.331730 Train acc =0.483000 valid loss =1.962573 valid acc =0.470000\n",
      "12 0 Train loss =1.670553 Train acc =0.443000 valid loss =2.024346 valid acc =0.447000\n",
      "12 1000 Train loss =1.072553 Train acc =0.617000 valid loss =1.258794 valid acc =0.571000\n",
      "12 2000 Train loss =1.183495 Train acc =0.654000 valid loss =1.074477 valid acc =0.664000\n",
      "13 0 Train loss =0.916214 Train acc =0.735000 valid loss =0.930822 valid acc =0.680000\n",
      "13 1000 Train loss =0.930905 Train acc =0.743000 valid loss =1.791180 valid acc =0.469000\n",
      "13 2000 Train loss =0.361867 Train acc =0.879000 valid loss =1.007501 valid acc =0.678000\n",
      "14 0 Train loss =0.600367 Train acc =0.821000 valid loss =0.792697 valid acc =0.789000\n",
      "14 1000 Train loss =0.415504 Train acc =0.862000 valid loss =0.539572 valid acc =0.825000\n",
      "14 2000 Train loss =0.454843 Train acc =0.853000 valid loss =0.682726 valid acc =0.788000\n",
      "15 0 Train loss =0.407822 Train acc =0.862000 valid loss =0.710006 valid acc =0.777000\n",
      "15 1000 Train loss =0.315250 Train acc =0.915000 valid loss =0.339605 valid acc =0.899000\n",
      "15 2000 Train loss =0.278888 Train acc =0.910000 valid loss =0.239072 valid acc =0.927000\n",
      "16 0 Train loss =0.297598 Train acc =0.901000 valid loss =0.277099 valid acc =0.921000\n",
      "16 1000 Train loss =0.264816 Train acc =0.924000 valid loss =0.446297 valid acc =0.870000\n",
      "16 2000 Train loss =0.134283 Train acc =0.959000 valid loss =0.165023 valid acc =0.955000\n",
      "17 0 Train loss =0.133122 Train acc =0.962000 valid loss =0.114659 valid acc =0.970000\n",
      "17 1000 Train loss =0.613339 Train acc =0.822000 valid loss =0.254012 valid acc =0.921000\n",
      "17 2000 Train loss =0.288853 Train acc =0.943000 valid loss =0.217275 valid acc =0.936000\n",
      "18 0 Train loss =0.189523 Train acc =0.945000 valid loss =0.214601 valid acc =0.932000\n",
      "18 1000 Train loss =0.124117 Train acc =0.962000 valid loss =0.114555 valid acc =0.973000\n",
      "18 2000 Train loss =0.104938 Train acc =0.971000 valid loss =0.155491 valid acc =0.952000\n",
      "19 0 Train loss =0.201035 Train acc =0.941000 valid loss =0.149893 valid acc =0.955000\n",
      "19 1000 Train loss =0.102010 Train acc =0.965000 valid loss =0.134727 valid acc =0.955000\n",
      "19 2000 Train loss =0.219635 Train acc =0.942000 valid loss =0.184250 valid acc =0.947000\n",
      "\n",
      "Optimization finished\n"
     ]
    }
   ],
   "source": [
    "# remove val_accuarcy- that will set it to default \n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    summary_writer = tf.train.SummaryWriter(logs_path,graph=tf.get_default_graph())\n",
    "    saver.restore(sess,os.getcwd()+model_path+\"_\"+\"17\"+\"_\"+\"2000\")\n",
    "    for ep in tqdm_notebook(range(epochs),desc=\"1st loop\"):\n",
    "        batches = int(len(trX)/batch_size)\n",
    "        x_train,y_train = shuffle(trX,trY,random_state=0)\n",
    "        for i in tqdm_notebook(range(batches),desc=\"2nd loop\", leave = False):\n",
    "            X_train, Y_train = x_train[i*batch_size:(i+1)*batch_size],y_train[i*batch_size:(i+1)*batch_size]\n",
    "            _,c,summary = sess.run([optimizer,cost,merged_summary_op],feed_dict={X : np.float32(X_train), Y : np.float32(Y_train)})\n",
    "            if i % 1000 == 0:\n",
    "                train_loss, train_acc = sess.run([cost,accuracy], feed_dict={X : np.float32(x_train[0:1000,:,:,:]), Y : np.float32(y_train[0:1000])})\n",
    "                valx,valy = shuffle(valX,valY, random_state=0)\n",
    "                valid_loss, valid_acc = sess.run([cost,accuracy], feed_dict={X : np.float32(valx[0:1000]), Y : np.float32(valy[0:1000])})\n",
    "                print (str(ep),str(i),\"Train loss =\"+\"{:.6f}\".format(train_loss),\\\n",
    "                       \"Train acc =\"+\"{:.6f}\".format(train_acc),\\\n",
    "                      \"valid loss =\"+\"{:.6f}\".format(valid_loss),\\\n",
    "                      \"valid acc =\"+\"{:.6f}\".format(valid_acc))\n",
    "                if max(val_acc) < valid_acc:\n",
    "                    val_acc.append(valid_acc)\n",
    "                    saver.save(sess,os.getcwd()+model_path+\"_\"+\"run2\"+\"_\"+str(ep)+\"_\"+str(i))\n",
    "    print (\"Optimization finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0.105,\n",
       " 0.38100004,\n",
       " 0.67199999,\n",
       " 0.88700002,\n",
       " 0.95100003,\n",
       " 0.95400012,\n",
       " 0.97400004,\n",
       " 0.97600001,\n",
       " 0.97899997,\n",
       " 0.97900009,\n",
       " 0.98299998]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reducing the learning rate and running more epochs \n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred,Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.00001).minimize(cost)\n",
    "\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 Train loss =0.112754 Train acc =0.972000 valid loss =0.092918 valid acc =0.971000\n",
      "0 1000 Train loss =0.092681 Train acc =0.977000 valid loss =0.151973 valid acc =0.968000\n",
      "0 2000 Train loss =0.076151 Train acc =0.974000 valid loss =0.099301 valid acc =0.969000\n",
      "1 0 Train loss =0.069196 Train acc =0.980000 valid loss =0.090019 valid acc =0.977000\n",
      "1 1000 Train loss =0.056951 Train acc =0.986000 valid loss =0.067025 valid acc =0.980000\n",
      "1 2000 Train loss =0.071047 Train acc =0.978000 valid loss =0.065641 valid acc =0.984000\n",
      "2 0 Train loss =0.164126 Train acc =0.961000 valid loss =0.093142 valid acc =0.973000\n",
      "2 1000 Train loss =0.120449 Train acc =0.967000 valid loss =0.099341 valid acc =0.978000\n",
      "2 2000 Train loss =0.097075 Train acc =0.972000 valid loss =0.104550 valid acc =0.968000\n",
      "3 0 Train loss =0.064887 Train acc =0.984000 valid loss =1.489470 valid acc =0.979000\n",
      "3 1000 Train loss =0.055873 Train acc =0.979000 valid loss =0.100839 valid acc =0.977000\n",
      "3 2000 Train loss =0.082534 Train acc =0.974000 valid loss =0.073970 valid acc =0.974000\n",
      "4 0 Train loss =0.037977 Train acc =0.987000 valid loss =0.093343 valid acc =0.979000\n",
      "4 1000 Train loss =0.065375 Train acc =0.980000 valid loss =0.068323 valid acc =0.981000\n",
      "4 2000 Train loss =0.057114 Train acc =0.979000 valid loss =0.047284 valid acc =0.987000\n",
      "5 0 Train loss =0.065989 Train acc =0.979000 valid loss =0.109245 valid acc =0.975000\n",
      "5 1000 Train loss =0.094080 Train acc =0.971000 valid loss =0.093280 valid acc =0.971000\n",
      "5 2000 Train loss =0.054393 Train acc =0.984000 valid loss =0.097758 valid acc =0.978000\n",
      "6 0 Train loss =0.070478 Train acc =0.981000 valid loss =0.086495 valid acc =0.978000\n",
      "6 1000 Train loss =0.078498 Train acc =0.973000 valid loss =0.068233 valid acc =0.979000\n",
      "6 2000 Train loss =0.078632 Train acc =0.976000 valid loss =0.061168 valid acc =0.980000\n",
      "7 0 Train loss =0.043128 Train acc =0.984000 valid loss =0.067688 valid acc =0.980000\n",
      "7 1000 Train loss =0.045275 Train acc =0.987000 valid loss =0.068045 valid acc =0.979000\n",
      "7 2000 Train loss =0.084288 Train acc =0.974000 valid loss =0.069626 valid acc =0.980000\n",
      "8 0 Train loss =0.057858 Train acc =0.981000 valid loss =0.089141 valid acc =0.977000\n",
      "8 1000 Train loss =0.056551 Train acc =0.983000 valid loss =0.062906 valid acc =0.983000\n",
      "8 2000 Train loss =0.087229 Train acc =0.976000 valid loss =0.061732 valid acc =0.982000\n",
      "9 0 Train loss =0.053748 Train acc =0.984000 valid loss =0.073232 valid acc =0.981000\n",
      "9 1000 Train loss =0.051186 Train acc =0.985000 valid loss =0.077933 valid acc =0.979000\n",
      "9 2000 Train loss =0.044654 Train acc =0.987000 valid loss =0.063812 valid acc =0.981000\n",
      "10 0 Train loss =0.044326 Train acc =0.986000 valid loss =0.074867 valid acc =0.981000\n",
      "10 1000 Train loss =0.063288 Train acc =0.978000 valid loss =0.083390 valid acc =0.975000\n",
      "10 2000 Train loss =0.079549 Train acc =0.974000 valid loss =0.095902 valid acc =0.980000\n",
      "11 0 Train loss =0.043037 Train acc =0.987000 valid loss =0.202804 valid acc =0.937000\n",
      "11 1000 Train loss =0.060368 Train acc =0.982000 valid loss =0.045255 valid acc =0.989000\n",
      "11 2000 Train loss =0.042401 Train acc =0.985000 valid loss =0.055561 valid acc =0.983000\n",
      "12 0 Train loss =0.078681 Train acc =0.977000 valid loss =0.070498 valid acc =0.986000\n",
      "12 1000 Train loss =0.086964 Train acc =0.979000 valid loss =0.075640 valid acc =0.978000\n",
      "12 2000 Train loss =0.062045 Train acc =0.976000 valid loss =0.064573 valid acc =0.985000\n",
      "13 0 Train loss =0.063609 Train acc =0.982000 valid loss =0.089396 valid acc =0.978000\n",
      "13 1000 Train loss =0.127409 Train acc =0.964000 valid loss =0.066470 valid acc =0.981000\n",
      "13 2000 Train loss =0.052471 Train acc =0.984000 valid loss =0.063556 valid acc =0.981000\n",
      "14 0 Train loss =0.059213 Train acc =0.977000 valid loss =0.033703 valid acc =0.992000\n",
      "14 1000 Train loss =0.057943 Train acc =0.981000 valid loss =0.049461 valid acc =0.984000\n",
      "14 2000 Train loss =0.042707 Train acc =0.985000 valid loss =0.074910 valid acc =0.975000\n",
      "15 0 Train loss =0.042937 Train acc =0.987000 valid loss =0.055326 valid acc =0.984000\n",
      "15 1000 Train loss =0.038970 Train acc =0.990000 valid loss =0.053472 valid acc =0.988000\n",
      "15 2000 Train loss =0.057757 Train acc =0.985000 valid loss =0.084522 valid acc =0.979000\n",
      "16 0 Train loss =0.039779 Train acc =0.985000 valid loss =0.046971 valid acc =0.989000\n",
      "16 1000 Train loss =0.052613 Train acc =0.983000 valid loss =0.054937 valid acc =0.983000\n",
      "16 2000 Train loss =0.063039 Train acc =0.986000 valid loss =0.057860 valid acc =0.986000\n",
      "17 0 Train loss =0.057688 Train acc =0.985000 valid loss =0.085185 valid acc =0.981000\n",
      "17 1000 Train loss =0.037706 Train acc =0.986000 valid loss =0.049705 valid acc =0.984000\n",
      "17 2000 Train loss =0.052569 Train acc =0.985000 valid loss =0.076098 valid acc =0.978000\n",
      "18 0 Train loss =0.050667 Train acc =0.980000 valid loss =0.055249 valid acc =0.986000\n",
      "18 1000 Train loss =0.062310 Train acc =0.978000 valid loss =0.049240 valid acc =0.987000\n",
      "18 2000 Train loss =0.050858 Train acc =0.984000 valid loss =0.053474 valid acc =0.985000\n",
      "19 0 Train loss =0.041591 Train acc =0.991000 valid loss =0.059476 valid acc =0.984000\n",
      "19 1000 Train loss =0.039939 Train acc =0.989000 valid loss =0.066219 valid acc =0.985000\n",
      "19 2000 Train loss =0.087739 Train acc =0.974000 valid loss =0.066936 valid acc =0.983000\n",
      "\n",
      "Optimization finished\n"
     ]
    }
   ],
   "source": [
    "# remove val_accuarcy- that will set it to default \n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    summary_writer = tf.train.SummaryWriter(logs_path,graph=tf.get_default_graph())\n",
    "    saver.restore(sess,os.getcwd()+model_path+\"_\"+\"run2\"+\"_\"+\"10\"+\"_\"+\"2000\")\n",
    "    for ep in tqdm_notebook(range(epochs),desc=\"1st loop\"):\n",
    "        batches = int(len(trX)/batch_size)\n",
    "        x_train,y_train = shuffle(trX,trY,random_state=0)\n",
    "        for i in tqdm_notebook(range(batches),desc=\"2nd loop\", leave = False):\n",
    "            X_train, Y_train = x_train[i*batch_size:(i+1)*batch_size],y_train[i*batch_size:(i+1)*batch_size]\n",
    "            _,c,summary = sess.run([optimizer,cost,merged_summary_op],feed_dict={X : np.float32(X_train), Y : np.float32(Y_train)})\n",
    "            if i % 1000 == 0:\n",
    "                train_loss, train_acc = sess.run([cost,accuracy], feed_dict={X : np.float32(x_train[0:1000,:,:,:]), Y : np.float32(y_train[0:1000])})\n",
    "                valx,valy = shuffle(valX,valY, random_state=0)\n",
    "                valid_loss, valid_acc = sess.run([cost,accuracy], feed_dict={X : np.float32(valx[0:1000]), Y : np.float32(valy[0:1000])})\n",
    "                print (str(ep),str(i),\"Train loss =\"+\"{:.6f}\".format(train_loss),\\\n",
    "                       \"Train acc =\"+\"{:.6f}\".format(train_acc),\\\n",
    "                      \"valid loss =\"+\"{:.6f}\".format(valid_loss),\\\n",
    "                      \"valid acc =\"+\"{:.6f}\".format(valid_acc))\n",
    "                if max(val_acc) < valid_acc:\n",
    "                    val_acc.append(valid_acc)\n",
    "                    saver.save(sess,os.getcwd()+model_path+\"_\"+\"run3\"+\"_\"+str(ep)+\"_\"+str(i))\n",
    "    print (\"Optimization finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0.105,\n",
       " 0.38100004,\n",
       " 0.67199999,\n",
       " 0.88700002,\n",
       " 0.95100003,\n",
       " 0.95400012,\n",
       " 0.97400004,\n",
       " 0.97600001,\n",
       " 0.97899997,\n",
       " 0.97900009,\n",
       " 0.98299998,\n",
       " 0.98399997,\n",
       " 0.98699999,\n",
       " 0.98899996,\n",
       " 0.99199998]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reducing the learning rate and running more epochs \n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred,Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.000001).minimize(cost)\n",
    "\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 Train loss =0.049085 Train acc =0.986000 valid loss =0.058823 valid acc =0.986000\n",
      "0 1000 Train loss =0.039224 Train acc =0.986000 valid loss =0.054207 valid acc =0.985000\n",
      "0 2000 Train loss =0.047641 Train acc =0.986000 valid loss =0.057346 valid acc =0.985000\n",
      "1 0 Train loss =0.062885 Train acc =0.982000 valid loss =0.068016 valid acc =0.987000\n",
      "1 1000 Train loss =0.088306 Train acc =0.975000 valid loss =0.074711 valid acc =0.983000\n",
      "1 2000 Train loss =0.071275 Train acc =0.980000 valid loss =0.086353 valid acc =0.977000\n",
      "2 0 Train loss =0.038848 Train acc =0.992000 valid loss =0.066262 valid acc =0.983000\n",
      "2 1000 Train loss =0.047143 Train acc =0.984000 valid loss =0.048696 valid acc =0.983000\n",
      "2 2000 Train loss =0.081746 Train acc =0.979000 valid loss =0.046983 valid acc =0.990000\n",
      "3 0 Train loss =0.047122 Train acc =0.983000 valid loss =0.098130 valid acc =0.984000\n",
      "3 1000 Train loss =0.052317 Train acc =0.983000 valid loss =0.062606 valid acc =0.986000\n",
      "3 2000 Train loss =0.050192 Train acc =0.984000 valid loss =0.060915 valid acc =0.984000\n",
      "4 0 Train loss =0.053307 Train acc =0.986000 valid loss =0.060836 valid acc =0.984000\n",
      "4 1000 Train loss =0.041396 Train acc =0.990000 valid loss =0.050787 valid acc =0.987000\n",
      "4 2000 Train loss =0.042519 Train acc =0.985000 valid loss =0.053716 valid acc =0.985000\n",
      "5 0 Train loss =0.046396 Train acc =0.983000 valid loss =0.062489 valid acc =0.982000\n",
      "5 1000 Train loss =0.042311 Train acc =0.987000 valid loss =0.055254 valid acc =0.985000\n",
      "5 2000 Train loss =0.043780 Train acc =0.986000 valid loss =0.086956 valid acc =0.977000\n",
      "6 0 Train loss =0.044913 Train acc =0.987000 valid loss =0.067444 valid acc =0.983000\n",
      "6 1000 Train loss =0.052764 Train acc =0.988000 valid loss =0.076553 valid acc =0.981000\n",
      "6 2000 Train loss =0.043377 Train acc =0.987000 valid loss =0.103102 valid acc =0.969000\n",
      "7 0 Train loss =0.041154 Train acc =0.985000 valid loss =0.071762 valid acc =0.981000\n",
      "7 1000 Train loss =0.055955 Train acc =0.983000 valid loss =0.094698 valid acc =0.979000\n",
      "7 2000 Train loss =0.041831 Train acc =0.987000 valid loss =0.072662 valid acc =0.986000\n",
      "8 0 Train loss =0.044504 Train acc =0.983000 valid loss =0.045976 valid acc =0.986000\n",
      "8 1000 Train loss =0.042110 Train acc =0.982000 valid loss =0.071143 valid acc =0.983000\n",
      "8 2000 Train loss =0.075237 Train acc =0.978000 valid loss =0.059468 valid acc =0.987000\n",
      "9 0 Train loss =0.043957 Train acc =0.987000 valid loss =0.063486 valid acc =0.985000\n",
      "9 1000 Train loss =0.087141 Train acc =0.975000 valid loss =0.063572 valid acc =0.984000\n",
      "9 2000 Train loss =0.064723 Train acc =0.982000 valid loss =0.058592 valid acc =0.988000\n",
      "10 0 Train loss =0.033171 Train acc =0.991000 valid loss =0.073349 valid acc =0.984000\n",
      "10 1000 Train loss =0.042508 Train acc =0.986000 valid loss =0.083449 valid acc =0.977000\n",
      "10 2000 Train loss =0.050054 Train acc =0.981000 valid loss =0.069779 valid acc =0.982000\n",
      "11 0 Train loss =0.042738 Train acc =0.985000 valid loss =0.058915 valid acc =0.987000\n",
      "11 1000 Train loss =0.041024 Train acc =0.989000 valid loss =0.096788 valid acc =0.981000\n",
      "11 2000 Train loss =0.076676 Train acc =0.975000 valid loss =0.079161 valid acc =0.982000\n",
      "12 0 Train loss =0.184364 Train acc =0.963000 valid loss =0.069875 valid acc =0.978000\n",
      "12 1000 Train loss =0.036897 Train acc =0.989000 valid loss =0.059933 valid acc =0.983000\n",
      "12 2000 Train loss =0.044010 Train acc =0.987000 valid loss =0.054974 valid acc =0.989000\n",
      "13 0 Train loss =0.064475 Train acc =0.977000 valid loss =0.066635 valid acc =0.986000\n",
      "13 1000 Train loss =0.035572 Train acc =0.992000 valid loss =0.066289 valid acc =0.980000\n",
      "13 2000 Train loss =0.072639 Train acc =0.974000 valid loss =0.057184 valid acc =0.982000\n",
      "14 0 Train loss =0.044393 Train acc =0.987000 valid loss =0.073065 valid acc =0.986000\n",
      "14 1000 Train loss =0.093360 Train acc =0.970000 valid loss =0.057757 valid acc =0.986000\n",
      "14 2000 Train loss =0.055140 Train acc =0.988000 valid loss =0.066933 valid acc =0.985000\n",
      "15 0 Train loss =0.059039 Train acc =0.986000 valid loss =0.083421 valid acc =0.976000\n",
      "15 1000 Train loss =0.062785 Train acc =0.979000 valid loss =0.071235 valid acc =0.981000\n",
      "15 2000 Train loss =0.059513 Train acc =0.983000 valid loss =0.062680 valid acc =0.981000\n",
      "16 0 Train loss =0.058404 Train acc =0.980000 valid loss =0.073223 valid acc =0.981000\n",
      "16 1000 Train loss =0.072073 Train acc =0.981000 valid loss =0.080190 valid acc =0.984000\n",
      "16 2000 Train loss =0.033795 Train acc =0.991000 valid loss =0.071546 valid acc =0.986000\n",
      "17 0 Train loss =0.032894 Train acc =0.989000 valid loss =0.062160 valid acc =0.986000\n",
      "17 1000 Train loss =0.038830 Train acc =0.982000 valid loss =0.048745 valid acc =0.986000\n",
      "17 2000 Train loss =0.047564 Train acc =0.985000 valid loss =0.064253 valid acc =0.984000\n",
      "18 0 Train loss =0.036497 Train acc =0.987000 valid loss =0.080225 valid acc =0.981000\n",
      "18 1000 Train loss =0.052522 Train acc =0.984000 valid loss =0.055523 valid acc =0.984000\n",
      "18 2000 Train loss =0.038232 Train acc =0.987000 valid loss =0.064983 valid acc =0.987000\n",
      "19 0 Train loss =0.078193 Train acc =0.976000 valid loss =0.095671 valid acc =0.973000\n",
      "19 1000 Train loss =0.053886 Train acc =0.984000 valid loss =0.059108 valid acc =0.981000\n",
      "19 2000 Train loss =0.039522 Train acc =0.989000 valid loss =0.094723 valid acc =0.976000\n",
      "\n",
      "Optimization finished\n"
     ]
    }
   ],
   "source": [
    "# remove val_accuarcy- that will set it to default \n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    summary_writer = tf.train.SummaryWriter(logs_path,graph=tf.get_default_graph())\n",
    "    saver.restore(sess,os.getcwd()+model_path+\"_\"+\"run3\"+\"_\"+\"14\"+\"_\"+\"0\")\n",
    "    for ep in tqdm_notebook(range(epochs),desc=\"1st loop\"):\n",
    "        batches = int(len(trX)/batch_size)\n",
    "        x_train,y_train = shuffle(trX,trY,random_state=0)\n",
    "        for i in tqdm_notebook(range(batches),desc=\"2nd loop\", leave = False):\n",
    "            X_train, Y_train = x_train[i*batch_size:(i+1)*batch_size],y_train[i*batch_size:(i+1)*batch_size]\n",
    "            _,c,summary = sess.run([optimizer,cost,merged_summary_op],feed_dict={X : np.float32(X_train), Y : np.float32(Y_train)})\n",
    "            if i % 1000 == 0:\n",
    "                train_loss, train_acc = sess.run([cost,accuracy], feed_dict={X : np.float32(x_train[0:1000,:,:,:]), Y : np.float32(y_train[0:1000])})\n",
    "                valx,valy = shuffle(valX,valY, random_state=0)\n",
    "                valid_loss, valid_acc = sess.run([cost,accuracy], feed_dict={X : np.float32(valx[0:1000]), Y : np.float32(valy[0:1000])})\n",
    "                print (str(ep),str(i),\"Train loss =\"+\"{:.6f}\".format(train_loss),\\\n",
    "                       \"Train acc =\"+\"{:.6f}\".format(train_acc),\\\n",
    "                      \"valid loss =\"+\"{:.6f}\".format(valid_loss),\\\n",
    "                      \"valid acc =\"+\"{:.6f}\".format(valid_acc))\n",
    "                if max(val_acc) < valid_acc:\n",
    "                    val_acc.append(valid_acc)\n",
    "                    saver.save(sess,os.getcwd()+model_path+\"_\"+\"run4\"+\"_\"+str(ep)+\"_\"+str(i))\n",
    "    print (\"Optimization finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0.105,\n",
       " 0.38100004,\n",
       " 0.67199999,\n",
       " 0.88700002,\n",
       " 0.95100003,\n",
       " 0.95400012,\n",
       " 0.97400004,\n",
       " 0.97600001,\n",
       " 0.97899997,\n",
       " 0.97900009,\n",
       " 0.98299998,\n",
       " 0.98399997,\n",
       " 0.98699999,\n",
       " 0.98899996,\n",
       " 0.99199998]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reducing the learning rate and running more epochs \n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred,Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.000001).minimize(cost)\n",
    "\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 Train loss =0.046073 Train acc =0.986400 valid loss =0.062409 valid acc =0.983600\n",
      "0 1000 Train loss =0.036124 Train acc =0.988600 valid loss =0.066961 valid acc =0.980400\n",
      "0 2000 Train loss =0.070132 Train acc =0.980200 valid loss =0.065996 valid acc =0.981600\n",
      "1 0 Train loss =0.053473 Train acc =0.983200 valid loss =0.067635 valid acc =0.980200\n",
      "1 1000 Train loss =0.040053 Train acc =0.988400 valid loss =0.061144 valid acc =0.985600\n",
      "1 2000 Train loss =0.041207 Train acc =0.986800 valid loss =0.094217 valid acc =0.975400\n",
      "2 0 Train loss =0.055789 Train acc =0.982600 valid loss =0.044099 valid acc =0.986400\n",
      "2 1000 Train loss =0.056655 Train acc =0.981800 valid loss =0.048756 valid acc =0.985000\n",
      "2 2000 Train loss =0.041522 Train acc =0.987000 valid loss =0.076343 valid acc =0.978200\n",
      "3 0 Train loss =0.063933 Train acc =0.980800 valid loss =0.047181 valid acc =0.987000\n",
      "3 1000 Train loss =0.057242 Train acc =0.980200 valid loss =0.058885 valid acc =0.985200\n",
      "3 2000 Train loss =0.102144 Train acc =0.972800 valid loss =0.055250 valid acc =0.983400\n",
      "4 0 Train loss =0.068626 Train acc =0.980400 valid loss =0.057938 valid acc =0.985200\n",
      "4 1000 Train loss =0.049750 Train acc =0.987000 valid loss =0.059353 valid acc =0.984000\n",
      "4 2000 Train loss =0.041915 Train acc =0.987000 valid loss =0.066273 valid acc =0.982200\n",
      "5 0 Train loss =0.104554 Train acc =0.968800 valid loss =0.061747 valid acc =0.983800\n",
      "5 1000 Train loss =0.052292 Train acc =0.985000 valid loss =0.043888 valid acc =0.985600\n",
      "5 2000 Train loss =0.050923 Train acc =0.983200 valid loss =0.055776 valid acc =0.983800\n",
      "6 0 Train loss =0.048756 Train acc =0.984200 valid loss =0.045322 valid acc =0.986800\n",
      "6 1000 Train loss =0.052235 Train acc =0.982200 valid loss =0.073014 valid acc =0.977200\n",
      "6 2000 Train loss =0.068647 Train acc =0.980000 valid loss =0.044361 valid acc =0.986800\n",
      "7 0 Train loss =0.040015 Train acc =0.987400 valid loss =0.060882 valid acc =0.983800\n",
      "7 1000 Train loss =0.061877 Train acc =0.982400 valid loss =0.119721 valid acc =0.966400\n",
      "7 2000 Train loss =0.038514 Train acc =0.988000 valid loss =0.065204 valid acc =0.981200\n",
      "8 0 Train loss =0.045887 Train acc =0.986200 valid loss =0.046319 valid acc =0.987000\n",
      "8 1000 Train loss =0.073908 Train acc =0.980200 valid loss =0.047839 valid acc =0.987600\n",
      "8 2000 Train loss =0.039583 Train acc =0.989600 valid loss =0.054825 valid acc =0.984400\n",
      "9 0 Train loss =0.071321 Train acc =0.979800 valid loss =0.064826 valid acc =0.982600\n",
      "9 1000 Train loss =0.055001 Train acc =0.982000 valid loss =0.097455 valid acc =0.971800\n",
      "9 2000 Train loss =0.059396 Train acc =0.983600 valid loss =0.045315 valid acc =0.985200\n",
      "10 0 Train loss =0.047353 Train acc =0.985200 valid loss =0.069175 valid acc =0.983000\n",
      "10 1000 Train loss =0.040557 Train acc =0.987200 valid loss =0.064087 valid acc =0.985800\n",
      "10 2000 Train loss =0.051502 Train acc =0.985800 valid loss =0.065283 valid acc =0.983000\n",
      "11 0 Train loss =0.047348 Train acc =0.985200 valid loss =0.063175 valid acc =0.982000\n",
      "11 1000 Train loss =0.054891 Train acc =0.984600 valid loss =0.042735 valid acc =0.987400\n",
      "11 2000 Train loss =0.059442 Train acc =0.984600 valid loss =0.051417 valid acc =0.984400\n",
      "12 0 Train loss =0.049291 Train acc =0.986000 valid loss =0.052692 valid acc =0.984400\n",
      "12 1000 Train loss =0.078860 Train acc =0.976400 valid loss =0.064610 valid acc =0.980200\n",
      "12 2000 Train loss =0.060767 Train acc =0.982200 valid loss =0.048110 valid acc =0.986400\n",
      "13 0 Train loss =0.062207 Train acc =0.981200 valid loss =0.070775 valid acc =0.978200\n",
      "13 1000 Train loss =0.059529 Train acc =0.982000 valid loss =0.053183 valid acc =0.983400\n",
      "13 2000 Train loss =0.058211 Train acc =0.980400 valid loss =0.057847 valid acc =0.984000\n",
      "14 0 Train loss =0.044026 Train acc =0.986400 valid loss =0.057278 valid acc =0.984400\n",
      "14 1000 Train loss =0.058416 Train acc =0.982000 valid loss =0.057441 valid acc =0.983400\n",
      "14 2000 Train loss =0.035939 Train acc =0.989400 valid loss =0.073260 valid acc =0.981200\n",
      "15 0 Train loss =0.034679 Train acc =0.986600 valid loss =0.057138 valid acc =0.981400\n",
      "15 1000 Train loss =0.049542 Train acc =0.983800 valid loss =0.068558 valid acc =0.982000\n",
      "15 2000 Train loss =0.054927 Train acc =0.982000 valid loss =0.065474 valid acc =0.981600\n",
      "16 0 Train loss =0.055129 Train acc =0.983600 valid loss =0.047327 valid acc =0.984600\n",
      "16 1000 Train loss =0.046010 Train acc =0.985200 valid loss =0.051164 valid acc =0.986000\n",
      "16 2000 Train loss =0.048404 Train acc =0.985000 valid loss =0.102723 valid acc =0.974400\n",
      "17 0 Train loss =0.064772 Train acc =0.982000 valid loss =0.069986 valid acc =0.978400\n",
      "17 1000 Train loss =0.051525 Train acc =0.983400 valid loss =0.056845 valid acc =0.984000\n",
      "17 2000 Train loss =0.046248 Train acc =0.986800 valid loss =0.070292 valid acc =0.980600\n",
      "18 0 Train loss =0.051301 Train acc =0.984000 valid loss =0.055282 valid acc =0.984000\n",
      "18 1000 Train loss =0.039263 Train acc =0.988200 valid loss =0.119625 valid acc =0.961200\n",
      "18 2000 Train loss =0.057266 Train acc =0.983400 valid loss =0.062578 valid acc =0.980800\n",
      "19 0 Train loss =0.044844 Train acc =0.986200 valid loss =0.049240 valid acc =0.985400\n",
      "19 1000 Train loss =0.045266 Train acc =0.986200 valid loss =0.064473 valid acc =0.984000\n",
      "19 2000 Train loss =0.088260 Train acc =0.977200 valid loss =0.041970 valid acc =0.987400\n",
      "\n",
      "Optimization finished\n"
     ]
    }
   ],
   "source": [
    "# remove val_accuarcy- that will set it to default \n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    summary_writer = tf.train.SummaryWriter(logs_path,graph=tf.get_default_graph())\n",
    "    saver.restore(sess,os.getcwd()+model_path+\"_\"+\"run3\"+\"_\"+\"14\"+\"_\"+\"0\")\n",
    "    for ep in tqdm_notebook(range(epochs),desc=\"1st loop\"):\n",
    "        batches = int(len(trX)/batch_size)\n",
    "        x_train,y_train = shuffle(trX,trY,random_state=0)\n",
    "        for i in tqdm_notebook(range(batches),desc=\"2nd loop\", leave = False):\n",
    "            X_train, Y_train = x_train[i*batch_size:(i+1)*batch_size],y_train[i*batch_size:(i+1)*batch_size]\n",
    "            _,c,summary = sess.run([optimizer,cost,merged_summary_op],feed_dict={X : np.float32(X_train), Y : np.float32(Y_train)})\n",
    "            if i % 1000 == 0:\n",
    "                train_loss, train_acc = sess.run([cost,accuracy], feed_dict={X : np.float32(x_train[0:5000,:,:,:]), Y : np.float32(y_train[0:5000])})\n",
    "                valid_loss, valid_acc = sess.run([cost,accuracy], feed_dict={X : np.float32(valX[0:5000,:,:,:]), Y : np.float32(valY[0:5000])})\n",
    "                print (str(ep),str(i),\"Train loss =\"+\"{:.6f}\".format(train_loss),\\\n",
    "                       \"Train acc =\"+\"{:.6f}\".format(train_acc),\\\n",
    "                      \"valid loss =\"+\"{:.6f}\".format(valid_loss),\\\n",
    "                      \"valid acc =\"+\"{:.6f}\".format(valid_acc))\n",
    "                if max(val_acc) < valid_acc:\n",
    "                    val_acc.append(valid_acc)\n",
    "                    saver.save(sess,os.getcwd()+model_path+\"_\"+\"run4\"+\"_\"+str(ep)+\"_\"+str(i))\n",
    "    print (\"Optimization finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "005593e1b078463bb9941317f65bb520": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "00ff1cfb31fb4ae4a25e7cf7f3caa8cd": {
     "views": [
      {
       "cell_index": 26
      }
     ]
    },
    "025d7aa56feb4dd49c337fd68ab498d4": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "026c204296bb4e55a8310dcc16104528": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "04996d7760ba49f19daaf11e8108e9c8": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    },
    "061bf341e36d48b882707ae495515553": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "068101d563494ed591c8194c89825af4": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "06d59c2fe8d84242a03c6c32d40d62cd": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "0737dad890e446aa93709ab535dc5321": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "07583c4b2a8c40429b0a3f3270cb0e7e": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    },
    "0849538872e043c6b00ccadc43215f16": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    },
    "0863375246ac48098f8afa2da8e3b73f": {
     "views": [
      {
       "cell_index": 26
      }
     ]
    },
    "09bd2e8c0c834630b54087474074c31c": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    },
    "09d74ac34ef847118927df05313be362": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "0ae7ab0a3a854062ba247b37729611c5": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "0b875e41fa774af08bef352a6e72691b": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "0bd71edd4b8d4066884ffb6592e6d461": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "0d5e5e98a9af431384a0d478429bab5b": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "0f14f838e7244146b99e9051c2c7cd5c": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "1221fa08b3ce4dcab44efcc50d2b05d0": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "123d0dc773014b728c3112fce116262e": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "14046ca9fdb64ec9a9d076d16daef7e2": {
     "views": [
      {
       "cell_index": 26
      }
     ]
    },
    "14d7d670a63944f2b896f7c9c7e2c351": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "15576d2aa21443d188017282834e991f": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    },
    "16268bfc29f049788d61e803c126e1b6": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "16bae39822c34822a3af14681fe18787": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    },
    "17dbdba7428a44bcb1f1b1137ad0d1d7": {
     "views": [
      {
       "cell_index": 26
      }
     ]
    },
    "17ff87fd4b344579bb756323f1a0441c": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    },
    "1807b8b2610740baa64c8e7f583509cc": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    },
    "19df845a88624ac9aa6d4fdb8b12e80b": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    },
    "1c07cea66804401db3c5006a3768d465": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    },
    "20fa0eaa076540a9bae47121110325d8": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "219e6453da2e406799465b21db0c3218": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    },
    "22c0ce84cbc6474197d78f59da15ec22": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "24dfc48a349d42e9bb07b1dbdafd6eb8": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "254ebb6cb9864b0d8ca4b458d8fd10b2": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "283547536e294a1d9df964968ec29fd3": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "29b40e31ed7b453f88622e95947b4d9d": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "2b6ecc3e3734425aa4ce0c3dec305af1": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "2bff9ee7ab6f4531abdcfa2962dc99bb": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    },
    "2c067bd9374341bf8e9b1a8693aa60f3": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "2d428e141cac430383e91ea9a6d8b038": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    },
    "2dd17e4f67ca459da3d8d681eab8eaf0": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "2e06b3c1527d40f3b89e189575c715fb": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "2e85c4b303844ce3a5567bb721594560": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "2eb68a11b868470b9b2a1f700d653d54": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "2ee42aad0d8442288a96b02621b54008": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "2fa7f926ecc64ba8b3e3bdb61cf757ee": {
     "views": [
      {
       "cell_index": 26
      }
     ]
    },
    "332df9d9aee040a59bfdffb06001dbe7": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    },
    "34d7032548f74901849be50ac67e856d": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "3533c1d0f85740dbad65be5597cc65a8": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    },
    "36839a62ade246a7aca9a4a80ac04bf3": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "37a1f3d421bb4d6f990e20231894889a": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "3aabae5e6da54e60a670bdd71a4d8cc8": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "3af781f5762341d38d0444e6b98dafdd": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    },
    "3c34d43a175442cda94243523806f1cc": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "3cc76c4fa31a45938ff749bab6641647": {
     "views": [
      {
       "cell_index": 26
      }
     ]
    },
    "3eea1cea7090484dac575953dd0d4fa2": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "3f0ddd37fad24bbe9055c6bbc4554711": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "3ffe1749c8014f4faae34e97f6302f88": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "427eb650af0c4f2f8ebc0ff7b87fef68": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "42dfd08b59fe40679e700e2e38082521": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    },
    "430c979f489a4bd7b57c23b8e6825b07": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "447db001b82645078125cf0c83e6d325": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    },
    "44ae2c1cdac543cebb942ba22a73953e": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "4c039c964db84971bf4e1d07cca19805": {
     "views": [
      {
       "cell_index": 26
      }
     ]
    },
    "4c2eb5ace4e541ae95fcd94422e8e4f0": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "4e6e46796a744f16b4ec65b6f849bb18": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    },
    "5076af79dd6044ae901fab1349f84e99": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "512ffa5bad184451a7886ade3739ce40": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "51ddc6791bc044b19c4e3eab8ee4febf": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "53dbf1ed8d1e4b90a85c2c265625950a": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "54187545139e458190b7be69170447a2": {
     "views": [
      {
       "cell_index": 26
      }
     ]
    },
    "556d1f385aed4faab32a60b7b57067d0": {
     "views": [
      {
       "cell_index": 26
      }
     ]
    },
    "55874263f4614bfba023e5e07b03513e": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "5603dfee2fd844f383eecda1ccc779f4": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "578d449412404187b9ba290d8d29e268": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "57b3ab754a7b45c4a4c762b60251191d": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "5c223c05fbb242c5b0d1c298acaa4616": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "5e98465c7a7244a3b9b68d3dd8496b18": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    },
    "5f679b33a1d84888ad28c37b838df08c": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "60ced54cb25746dea95575ed9c3e64ee": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "6112d94865b64cd6878b76fcd1ddb6d3": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    },
    "613abf299af1444e860bdeda2101618d": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "642657f94d7a4668b2871056198bd7bd": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    },
    "64fbab61b2b44c9c8f70191e6d4f9203": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "65ac558926b1409ea60f606926916846": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "684ffb2e1c6d4385b2e89fbd85a16440": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "68e5ed16a61949ec809d05d9bc9aed29": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "6af8bbb4e656477eb40ccfaf3c3aa918": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "6b7a606f153a4defb9cf2a9aeb5e87d5": {
     "views": [
      {
       "cell_index": 26
      }
     ]
    },
    "6b8a08a55dd84591aa2d8948b8e9ec9a": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "6c668e52cef54a8db4f9482bbfe73a04": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    },
    "6d19b4580a5a41eea1f044e611ae28ab": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    },
    "6f795bc99b7043268fa0a3c27b973e40": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "71b0683536b04c82b2cca093d3183790": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "72a8ea91fec646b3ae6b10b42aa984a3": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    },
    "74bd9e3f10c04a3d8b89a4aae0760645": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "79aba5ec9a644ce3b3de174d69038cb4": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "7aa8266ae1b94c108996f227a7bf8a9b": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "7b32e40cd95046d9885243c8e4d29391": {
     "views": [
      {
       "cell_index": 26
      }
     ]
    },
    "7fdb8c3a3e4f42e3bff5b4fd04ae6727": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "81829d901af04b7c93950ac6098dc68e": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "81d87df0b78a4f3e989a2598b36f4b08": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    },
    "8243eecbff884c62b5b8b737577a97db": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    },
    "82b59b9afdb8400eabffd0e36cfcb505": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    },
    "83b758432ed74946997c52ccb22c77e4": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "844e0ee2a90b4b46a61572ef1aef683e": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "891f94e20df0441c9acafbfd41019105": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "8b62de23c3554a848bde9c0662457f59": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    },
    "8ba66d91ffc340c6b2ba8972bbc3ab52": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    },
    "8bde3d23288242aaa383c88c59013f7a": {
     "views": [
      {
       "cell_index": 26
      }
     ]
    },
    "8c7815f2e5fe45c6ab43c0a4cca500ab": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "8d54cee05fb044f38ccd1b38e14ce42b": {
     "views": [
      {
       "cell_index": 26
      }
     ]
    },
    "90e34ded7c5c4762a2444d31ca6b37ec": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "90fee4c0824b4b1eb3d63c47dace43da": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "9141ad9ad39341c9a43f1fc4d5ccad60": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    },
    "94cf03f86fcb4cf4b543285218a0661f": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "9651eb2acfd042fe863f95de88786e95": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "968e1392467d4d94b7ad1396866c3360": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    },
    "97672b2a0d6d43328bf183a5c2659f72": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    },
    "99c20359cb5249d3bf30ee67980def62": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "9b5dd3ced6b24f1aaf812365f79c5822": {
     "views": [
      {
       "cell_index": 26
      }
     ]
    },
    "9c3d83b7d56b4689aa2430b91bcf2f0d": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "9c7941985036498d8d884886c16e3a49": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "9cfd7ce4c9f7431d9fc20bb98b83090f": {
     "views": [
      {
       "cell_index": 26
      }
     ]
    },
    "a10cb19132b740b789ae4dbc1a9e8c6d": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "a24ad34b69cb4cfaabbce12cf8e91865": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "a3dd7b4d657d461db5bf3475b2e3f765": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "a5229d234dec452d977052772555df48": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "a524a3b66e8d4e93bac67ebf6b73e4f5": {
     "views": [
      {
       "cell_index": 26
      }
     ]
    },
    "a71d9567d7534d019a81ec736ddc8903": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "a90db8bc899a450498c47e87b1bb4656": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    },
    "a9abab9eaecc4735977b6a5b12cce119": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "ac2b9b0213744c32b0cd0e4dff4f4b8f": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "acd107c276db4da1a98e41e835fe4718": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    },
    "ad9cb068a29b41c192b68835731418c9": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    },
    "ae3edf76c87446048cf770b72d8db69f": {
     "views": [
      {
       "cell_index": 26
      }
     ]
    },
    "b0b9255db9c2454ca11ec89741ef2a4a": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "b13dd5b2fcf0491fae017fca6885d25e": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "b1d62dfe845c4968b096d9ecbfb85f82": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "b35b125c5d2842e68bc0fb2cd04b6efb": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    },
    "b3e4045e365c4482ab3a4c8295f7d931": {
     "views": [
      {
       "cell_index": 26
      }
     ]
    },
    "b51636c614034d0495193735f793cbfe": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "b581241853094b9f81c352f60947b114": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "bab2dabb8cc94e4ca28689ef7ab1f9ec": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "bca91fcb74cb467aa9f5f0d08b7c06de": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "be2eac790368430cb3917dab5c888ab9": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "bed317258d5142db885f517af27550f7": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "c117a7c247be49ec958c95c968482571": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    },
    "c12dcfa248fd48deaeeb7e4f02e6fa97": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "c532578564144cdca8316833c75822ff": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    },
    "c99ce0d6ff394c91986fa80b8faa71b2": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "cbfe6ffd615741c28ddff6abeb30090b": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "ccad111296f04465bba217ad7d98b3a8": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "ce5f21781c684f738293381bcd00d367": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "cf47705c5d874564a90864c2bd31d7ca": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "cf87905fdbba45b8a2febe750e5d39fe": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "cf9ab2f57e01423f93266ecd5eab11dd": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "d0ea08dc8d7548e59e78b0f7c698a091": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "d46b0080c8854a0dbbf724d7956ff320": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "d6804dbaba444d839c2f75cc48fc1591": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "d80878b1827e4e279c3ec26d87fde406": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "d8e389a9eac34d20941af811664f9c36": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    },
    "da6905f2eb6a42e1a1be093c58f52970": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "da74019b19e04ba3bae9969e85985a35": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "dc6586034baa4c03afdba167b977fc86": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "dc6c7a47edd14740bb4f31cd3a86b855": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "dc8fe519c3fb4ed78cd0fa585f7f1b38": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    },
    "dce170a128be40ba9431de7cb12df7ee": {
     "views": [
      {
       "cell_index": 26
      }
     ]
    },
    "ded9acbe58ab40cd8ec86b50a0ebf4aa": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "dff555ee31674c8f92ec10928193cdbc": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    },
    "e1457664f8ff4db995fd4462e55bcf39": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "e150f505c5af43a3bdc19f046d860497": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "e1657d5e9aa14348ad9161a34fcc5ded": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "e32d135e7dab4eeb8bfe19921ed23e68": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "e436512fef3d43d687ed9766885eb012": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    },
    "e477530d0bc74bb0bd77da7fe1162782": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    },
    "e622e2dd1878483c9760375fcf50feac": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "e63cd629de114dd9ad952fe2c15557f0": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "e7d190cf1b924ab195dc63d8080fc1c4": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "e8aa4b4b2de84ced9084d124b6df9b14": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "ea2b56e489674ea599f0c74dbe39876d": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "eb356fd268f2430bbaf0e530419c47ec": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    },
    "edf22daf690b4ba39f7987c2da449d03": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "eea350b02f8d47f89bcd8b1717a4eb84": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "eeb79babcbae408e9084f30193310545": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "eefbbb23760d40b2847bcd8fefa60a4d": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "efaf48af3eff4de39451adee844f051d": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    },
    "f28c91ff262449ed80164e80d6d74421": {
     "views": [
      {
       "cell_index": 26
      }
     ]
    },
    "f366bb7e3da547f5bb1d1e722e1a5bc0": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "f54796241f0c45738b4ff5bb901cf105": {
     "views": [
      {
       "cell_index": 26
      }
     ]
    },
    "f7127c83699142fc821d2555bf9e1277": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "f8d70fcbd2bc4623ae09cb0dc2218902": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "f9bc648766274f7cb8e28ff6e29332ed": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "fa64441c40a34f4598fc9ac47d3068bd": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "fde70bbe74ec401c84e514265d72a04f": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "fe123e7d84884ddf9928a2acbefcc887": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
